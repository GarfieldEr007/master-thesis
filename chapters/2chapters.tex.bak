
\chapter{多任务学习方法}

\section{引言}

传统的机器学习方法中大多是把一个对象看做一个任务，每次只研究一个任务，这些方法称为单任务学习。例如在行为识别领域中，把每一类行为看做任务，每次仅对一个任务单独学习其对应的分类器。这种方法的缺点在于它忽略了隐藏在训练样本之间的丰富的可用信息，且在实际应用中，由于人力和经济等多种因素的制约，我们可以得到的训练样本是十分有限的，在这种训练样本不充分的情况下运用单任务学习得到的模型很可能存在过拟合等问题。通过观察可以发现，不仅仅是在数据库中，在现实生活中要学习的任务往往是相关联的。比如行为类别之间存在着许多相关的信息。如图~\ref{p2}~所示，行为骑行和骑马的视觉特征非常相似，那么计算机可以借助一些骑马的特征去识别骑行。如果将这些被传统的单任务学习忽略的潜在信息加以利用，那么便可以建立更好的分类模型并提高模型的泛化能力。基于此，Rich Caruana~于~1997~年率先提出了多任务学习方法（Multi-task Learning）~\cite{Caruana97}。

多任务学习被提出后受到了广泛的关注和研究，目前它已经成功的运用到了很多的领域，例如：物体识别、语音识别、手写体识别和疾病进展预测等。在行为识别领域，多任务学习通过将每一个行为类别看做一个任务，通过利用行为类别之间潜藏的关系来提高识别的精度。下文会对多任务学习框架进行概述并详细介绍几种常用的多任务学习模型。

\section{多任务学习方法概述}

\begin{figure*}
\begin{center}
\includegraphics[width=1\linewidth]{3.eps}
\end{center}
\caption{多任务学习与单任务学习的主要流程。}
\label{p3}
\end{figure*}

多任务学习方法建立在一个普遍的假设之上，即多个任务之间共享相同的特征空间或者模型。在此假设上，多任务学习通过利用任务之间的潜藏信息来学习到更好且更具有泛化能力的模型。如图~\ref{p3}~所示，它与单任务学习的主要区别在于后者忽略了任务之间的关系，每次只学习一个模型。而多任务学习不仅仅考虑了任务之间的差异性，还考虑了任务的相关性。多任务学习特别适用于训练数据较少的情况，它通过利用数据之间的相关性来提高学习的性能。因此，多任务学习在计算机视觉领域得到了广泛的应用，它研究的重点在于如何寻找任务之间的关联性，以及如何将关联性与模型结合。

1997~年，Rich Caruana~\cite{Caruana97}~正式提出了多任务学习这一概念。他构造了一个前馈神经网络，这个网络中的每一个任务对应着一个输出节点，而所有任务都共享了隐藏节点，输入节点到隐藏节点权重参数的训练过程中利用了任务之间的潜在关联信息，而隐藏节点到输出节点的权重参数包含了每个任务特有的信息。尽管这个方法比较简单，但是却成功应用到了医疗诊断和车辆自动驾驶等领域。与共享参数类似的方法还有学习分层的贝叶斯模型参数~\cite{LawrenceP04}、高斯过程~\cite{BakkerH03}的参数等。另一方面任务之间分享的不一定是参数，也可能是隐含的相同的特征空间。Evgeniou~和~Pontil~\cite{EvgeniouP04} 假设所有的任务都是相关联的且都可以投射到一个共同的未知的空间中，并提出了一种多任务学习的正则化框架，通过基于~SVM~的方法将其转为传统的单任务学习求解。Andreas Argyriou~\cite{ArgyriouEP06} 将~$\ell_{2,1}$~范数正则项与多任务学习结合，$\ell_{2,1}$~范数正则项可以鼓励多个任务学习到相同的稀疏特征空间。他们首先对所有任务对应的参数建立参数矩阵，然后在~$\ell_{2,1}$~范数的约束下最小化所有任务的经验风险值。

上述所有多任务学习方法都假设了数据库中的所有任务都是相关联的，然而这个假设对于现实的应用而言太过强烈，例如可能会存在一两个异常的类别，或如图~\ref{p2}~所示，类别之间的关系也是存在结构的。简单的假设所有任务共享所有的参数或共享一个特征空间可能会对模型的学习产生负面的影响。基于此，研究者们最近提出了许多任务学习方法来挖掘任务之间的潜在结构。Jacob~等人~\cite{JacobBV08}考虑通过对任务进行自动聚类来确定它们之间的关联性，在同一个集群中的任务比不在同一个集群中任务的相关程度更高。在他们的工作中，任务之间的关联性被建模为参数的相似度。Zhuoliang Kang~\cite{KangGS11} 同样研究了任务的聚类问题，与~Jacob~工作的不同之处在于他们把任务之间的关联性建模为任务之间分享的特征。他们提出的算法可以同时学习任务之间潜在的结构和每个任务的分类参数。Pinghua Gong~等人~\cite{GongYZ12}考虑到任务中异常点（outlier）的存在，于是提出了鲁棒的多任务特征学习（Robust Multi-Task Feature Learning, RMTFL），他们通过将分类参数矩阵分为两个组成部分，通过梯度下降法同时对相关联任务和异常任务学习分类器，并在理论上证明了该方法的可行性。Dinesh Jayaraman~等人~\cite{JayaramanSG14}利用任务的先验信息对任务进行了分组，并将分组结构信息作为正则项与多任务学习框架结合来进行属性分类。上述的方法都假设了数据库中的类别可以聚类为不相交的组，每个组之里的任务可以投射到同一个维度的子空间，然而Abhishek Kumar~等人~\cite{KumarD12}认为上述假设对于真实世界的情况仍旧不太现实，因为位于不同组内的任务是不可能一点关系都没有的。所以他们通过学习一些隐任务来描述不同组内的任务之间共享的信息。

\section{常用的多任务学习方法}

\subsection{基于~$\ell_{2,1}$~范数的多任务学习}
基于~$\ell_{2,1}$~范数的多任务学习是非常经典和常用的一种多任务学习方法。它首先由Andreas Argyriou~等人~\cite{ArgyriouEP06}提出，之后被运用到了计算机视觉的很多领域，例如物体识别和人脸识别等。他们认为所有的任务都是相关的，且可以投影到一个被所有任务共享的低维特征空间。他们将可以为单任务产生稀疏性的~$\ell_1$~范数扩展到了多任务的情况下。Andreas Argyriou~通过将该问题转化为等效的凸对偶形式从而优化求解。

\begin{figure*}
\begin{center}
\includegraphics[width=0.6\linewidth]{4.eps}
\end{center}
\caption{基于~$\ell_{2,1}$~范数的多任务学习为所有的任务同时学习到一个低维的特征空间，无色的方块代表任务对于特征的权重为~0~。}
\label{p4}
\end{figure*}

基于~$\ell_{2,1}$~范数的基本思想如图~\ref{p4}~所示，所有的任务都选择了相同维度的特征。假设数据库中有~$C$~个需要学习的任务，对于第~$c$~个任务有~$N_c$~个训练样本
~$\left(\mathbf{x}_{c1},y_{c1}\right),...,\left(\mathbf{x}_{cN_c},y_{cN_c}\right) \in \mathbb{R}^d \times \mathbb{R}$~，
该方法希望基于训练数据可以学习到~$C$~个分类或者回归函数~$f_c:\mathbb{R}^d \rightarrow \mathbb{R}$~。函数~$f_c$~可以表示为：

\begin{equation}
\label{f1}
  f_c(x) = \sum_{i=1}^d w_{ic}x_i,
\end{equation}
其中~$w_{ic}$~表示函数~$f_c$~对于第~$i$~维特征的分类或回归参数，$x_i$~表示样本~$x$~的第~$i$~维特征。基于~$\ell_{2,1}$~范数的多任务学习算法通过让一些~$w_{ic}$~为~0~来达到选择特征的目的。则目标函数为：

\begin{equation}
\label{f2}
  \min \sum_{i=1}^{N_c} L(y_{ci},\mathbf{w}_{c}\mathbf{x}_{ci}) + \gamma \parallel \mathbf{w}_c\parallel_1^2,
\end{equation}
其中~$\mathbf{w_c}\in \mathbf{R}^d$~为~$f_c$~的参数向量，$\gamma$~为正则项参数。上式中用到的~$\ell_1$~范数会为~$\mathbf{w_c}$~产生稀疏的表示，将公式~\eqref{f2}~扩展到多任务的情形下，可以得到目标函数为：

\begin{equation}
\label{f3}
  \min \sum_{c=1}^C\sum_{i=1}^{N_c} L(y_{ci},\mathbf{w}_{c}\mathbf{x}_{ci}) + \gamma \parallel \mathbf{W}\parallel^2_{2,1},
\end{equation}
公式~\eqref{f3}~是所有任务的损失函数，可以选择任意的一种具体损失函数来代替~$L$~。~$\mathbf{W} \in \mathbf{R}^{d \times C}$~表示所有任务的参数矩阵，第二项是对分类矩阵~$\mathbf{W}$~的约束。它对矩阵~$\mathbf{W}$~的每一行计算~$2$~-norm，再对所得的结果计算~$1$~-norm。基于该方法的思想，学习到的~$\mathbf{W}$~应该满足某一行全为~$0$~或不全为~$0$~，如图~\ref{p4}~所示。全为~$0$~的行表示这一维特征对所有的任务都不重要，所以被抛弃。

基于~$\ell_{2,1}$~范数的多任务学习应用广泛，它特别适用于训练样本较少的情况，这时它可以充分的利用样本之间的信息提高性能和泛化能力。

\subsection{基于组约束的多任务学习}

基于~$\ell_{2,1}$~范数的多任务学习可以为所有任务学习到一个低维的特征空间，但是由于它建立在所有任务都相关的假设之上，导致了它不适用于绝大多数的现实问题。基于此，研究者们提出了一些基于组约束的多任务学习方法。即任务可以根据相似程度分为若干个组，位于同一个组内的任务分享相同的特征空间。下面将介绍几种典型和常用的基于组约束的多任务学习方法。

\Exe Pinghua Gong~\cite{GongYZ12}~于~2012~年提出了鲁棒的多任务特征学习（RMTFL)，他认为数据库中大部分任务都是彼此相关联的，而那些不相关的任务称为异常任务，如图~\ref{p5}~所示。如果按照基于~$\ell_{2,1}$~范数的多任务学习忽略异常任务的存在，那么异常任务反而会降低模型的性能。RMTFL~可以同时分别为相关联的任务和异常任务学习到各自的特征空间。并且此方法不需要预先知道哪个任务是异常的，而是根据数据的特征自动的将它学习出来。RMTFL~将所有任务的参数权重矩阵~$\mathbf{W}$~分解成了~$\mathbf{P}$~和~$\mathbf{Q}$~两个元素之和，对~$\mathbf{P}$~加以~$\ell_{2,1}$~范数的约束使得相关的任务学习到相同的低维特征空间，同时也对元素~$\mathbf{Q}$~的转置也加以~$\ell_{2,1}$~约束从而学习到异常任务对应的模型参数，RMTFL~具体的学习框架如下所示：
\begin{equation}
\label{f4}
\begin{split}
    \min \sum_{c=1}^C\sum_{i=1}^{N_c} L(y_{ci},\mathbf{w}_{c}&\mathbf{x}_{ci}) + \lambda_1 \parallel \mathbf{P}\parallel^2_{2,1} +  \lambda_2 \parallel \mathbf{Q}^T\parallel^1_{2,1},\\
    &s.t.~\mathbf{W} = \mathbf{P} + \mathbf{Q},
\end{split}
\end{equation}
如公式~\eqref{f4}~所示，第二个关于~$\mathbf{P}$~的正则项为相关任务学习到相同的特征空间，第三个关于~$\mathbf{Q}$~的正则项可以发现异常任务。参数~$\lambda_1$~和~$\lambda_2$~分别控制两个正则项的惩罚力度。事实上通过观察可以发现，公式~\eqref{f4}~的前两项其实就是公式~\eqref{f3}~的变形，正如~2.3.1~小节所述，对元素~$\mathbf{P}$~加以~$\ell_{2,1}$~的约束可以使~$\mathbf{P}$~的行包含全~$0$~ 或者全不为~$0$~的元素，从而为所有相关的任务学习到相同的特征空间。然而由于异常任务的存在，假设所有的任务都共享同一个特征空间不太现实，所以又引入了~\eqref{f4}~中的第三项。通过对元素~$\mathbf{Q}$~的转置加以~$\ell_{2,1}$~的约束可以使~$\mathbf{Q}$~的列为全~$0$~或全不为~$0$~，而全部不为~0~的那一列就对应着异常任务对于所有特征维度的权重系数。如图~\ref{p6}~所示，如果~$\mathbf{Q}$~ 的第~$i$~列全不为~$0$~，那么对应的~$\mathbf{W}$~的矩阵也应当全不为~$0$~，这样第~$i$~个任务作为一个异常任务不与其他任务共享特征空间，同时~$\mathbf{Q}$~ 中其他全为~$0$~的列代表了这些任务彼此相关，而它们的模型参数可以通过求解~$\mathbf{P}$~ 得到。
\begin{figure}
  \centering
  \subfigure[ ]{
    \label{p5} %% label for first subfigure
    \includegraphics[width=1.9in]{5.eps}}
  \hspace{1in}
  \subfigure[ ]{
    \label{p6} %% label for second subfigure
    \includegraphics[width=3in]{6.eps}}
  \caption{(a)在大部分真实情况下，不一定是所有任务都彼此关联，总有少数异常任务的存在，他们不与其他任务共享特征空间。(b)RMTFL~的矩阵分解示意图。方块代表了某一个任务对于特征的权重，白色为~$0$~，彩色为非~$0$~。}
  \label{fig:subfig} %% label for entire figure
\end{figure}

\Exe Abhishek Kumar~\cite{KumarD12} 提出了一种新的基于组约束的多任务学习算法。众所周知多任务学习建立在任务之间分享信息的基础之上，那么可以直接把任务之间共享的信息学习出来，Abhishek Kumar~把任务之间共享的信息称之为基任务，并假设每一个任务可以表示为基任务的线性组合。这个算法与~RMTFL~等算法的优越性在于，RMTFL~认为相关任务群和异常任务不分享任何信息，然而对于现实情况而言任务与任务之间完全没有关系是很少见的，这个算法通过学习到有限的基任务再将任务表示为基任务的组合，可以使不同任务之间按亲疏程度共享一定数目的信息。如果两个任务关系密切，那么选中的相同的基任务的数量也会比较多。该算法将所有任务对应的参数矩阵~$\mathbf{W}$~表示为~$\mathbf{L}$~和~$\mathbf{S}$~两个元素之积，~$\mathbf{L} \in \mathbb{R}^{d \times k}$~的每一列都表示基任务，~$\mathbf{S} \in \mathbb{R}^{k \times C}$~表示了~$C$~个组合分别对于基任务的权重，具体框架如下所示：
\begin{equation}
\begin{split}
\label{f5}
    \min \sum_{c=1}^C\sum_{i=1}^{N_c} L(y_{ci},\mathbf{w}_{c}&\mathbf{x}_{ci}) + \lambda_1 \parallel \mathbf{S}\parallel_1 +  \lambda_2 \parallel \mathbf{L}^T\parallel^2_F,\\
    &s.t. ~\mathbf{W} = \mathbf{LS},
\end{split}
\end{equation}
如上式所示，第二项通过对~$\mathbf{S}$~加以~$\ell_1$~的约束，可以使~$\mathbf{S}$~矩阵的每一列产生稀疏表示，从而鼓励每一个任务可以仅被最重要和包含最多信息的基任务表示。$\parallel \mathbf{L}^T\parallel^2_F$~可以防止矩阵~$\mathbf{L}$~过拟合，它的每一列都代表了被~$d$~维特征所描述的基任务。矩阵~$\mathbf{S}$~中的任意两行~$\mathbf{s}_{c1}~$与~$\mathbf{s}_{c2}~$对应着任务~$c1$~和任务~$c2$~ 选中的基任务。$\mathbf{s}_{c1}$~与~$\mathbf{s}_{c2}$~的相似程度代表了这两个任务的相似程度。如果~$\mathbf{s}_{c1}$~与~$\mathbf{s}_{c2}$~ 具有完全相同的稀疏形式，说明任务~$c1$~和任务~$c2$~属于同一组。如果~$\mathbf{s}_{c1}$~与~$\mathbf{s}_{c2}$~ 的稀疏形式部分重叠，说明任务~$c1$~ 和任务~$c2$~ 属于不同的组，但是允许关联性不是很大的任务之间也分享特征信息。如果一个任务不与其他任何任务共享基任务，那么这个任务就是RMTFL的异常任务，如图~\ref{p7}~所示,~$\mathbf{S}$~中也蕴藏了数据库中任务潜在的结构信息。

公式~\eqref{f5}~在~$\mathbf{S}$~和~$\mathbf{L}$~分别固定的情况下是凸函数，所以可以使用交替迭代的算法来求解~$\mathbf{S}$~和~$\mathbf{L}$~的局部最小值。
\begin{figure*}
\begin{center}
\includegraphics[width=0.8\linewidth]{7.eps}
\end{center}
\caption{将权重矩阵~$\mathbf{W}$~表示为矩阵~$\mathbf{L}$~和矩阵~$\mathbf{S}$~的积。~$\mathbf{L}$~表示了任务之间分享的基任务，~$\mathbf{S}$~包含了数据库中任务潜在的结构信息。}
\label{P7}
\end{figure*}

Qiang Zhou~\cite{ZhouWJZ13} 等人在上述方法的基础上做出了改进，并将此多任务学习方法用于了行为识别。这里任务之间分享的基任务可以视为不同的行为类别之间共享的运动元素。比如跑步、走路、挥手等运动其实都是有身体部位的基本移动构成的。最终每个类别的分类器参数由基本运动元素线性组合而成。具体框架如下所示：
\begin{equation}
\begin{split}
\label{f6}
    \min \sum_{c=1}^C\sum_{i=1}^{N_c} L(y_{ci},\mathbf{w}_{c}\mathbf{x}_{ci}) + &\lambda_1 \parallel \mathbf{S}\parallel_1 +  \lambda_2 \parallel \mathbf{L}^T\parallel^2_F + \lambda_3 \parallel \mathbf{L}\parallel_1, \\
    &s.t.~\mathbf{W} = \mathbf{LS},
\end{split}
\end{equation}
与公式~\eqref{f5}~的差别在于，公式~\eqref{f6}~引进了对于基任务~$\mathbf{L}$~的~$\ell_1$~约束。因为在行为识别中，基任务应该对应的是被行为类别之间共享最多的运动元素，如果全局信息太多的话反而会降低判别性。对~$\mathbf{L}$~矩阵加以~$\ell_1$~的约束使得~$\mathbf{L}$~的行具有稀疏表示，而不为~$0$~的项对应着最重要的基本运动元素，这也是一种特征选择。

Qiang Zhou~\cite{ZhouWJZ13} 运用加速近似梯度下降算法的求得公式~\eqref{f6}~后，实验结果表明了与单任务学习相比多任务学习可以有效的提高行为识别的精度。这说明了行为类别之间分享着许多的信息，这也从侧面证明了本论文的工作的必要性。

\Exe 上述讨论的任务之间的结构都是单层的，Seyoung Kim~\cite{KimX10} 等人认为任务之间可能存在多层的结构，比如图~\ref{p8}~所示的树结构或者图结构。基于此，他们提出了树形结构正则项和基于树形结构约束的多任务学习。与上文中介绍的方法相似，基于树形结构约束的多任务学习也允许任务在多重的粒度上分组，即允许关联性不是很大的任务之间也分享特征信息，但是基于树形结构约束的多任务学习不会出现由于分享信息的重叠导致的惩罚过度的问题。如图~\ref{p8}~所示，假设任务之间的关系可以表示为拥有~$V$~个节点的树形结构。叶子节点代表任务，中间节点代表若干个任务构成的组，同属于一个节点的任务之间的相似度更高且被鼓励共享特征信息，据树底越近的中间节点表示它拥有的子树的关系越密切。为了量化每个组的关联程度，Seyoung Kim~给每一个节点~$v$~都赋以权重~$s_v$~，高度越低的节点的~$s_v$~越小。将此树形结构用数学的形式表达即可得到树形结构正则项：
\begin{equation}
\begin{split}
\label{f7}
     \sum_{j}\sum_{v \in V} s_v\parallel \mathbf{w}^j_{G_v}\parallel_2,
\end{split}
\end{equation}
其中~$\mathbf{w}^j_{G_v} = {\mathbf{w}_k^j:k \in G_v}$~表示属于第~$v$~组中所有任务对于第~$j$~维特征参数向量。而~$s_v$~ 保证了不会有参数被过度惩罚。在实际的运算中组的结构和每个节点对应的权重可以通过层次聚类算法得到。将树形结构正则项与多任务学习结合可以使任务选择的特征结构也呈对应的树形结构。如图~\ref{p9}~所示，假设输入的数据库中有~3~个任务，且通过树形聚类可得到~4~个分组，即~$G_{v1} = {\mathbf{w}_1},G_{v2} = {\mathbf{w}_2},G_{v3} = {\mathbf{w}_3},G_{v4} = \{\mathbf{w}_1,\mathbf{w}_2\}$~，其中~$\mathbf{w}$~ 为各个任务对应的分类参数，学习到的模型参数矩阵也对应着树形结构，即首先任务~1~和任务~2~分享特征，而任务~1~、 任务~2~、任务~3~之间又互相竞争和相斥，基于树形结构约束的多任务学习具体框架如下所示：
\begin{equation}
\begin{split}
\label{f8}
    \min \sum_{c=1}^C\sum_{i=1}^{N_c} L(y_{ci},\mathbf{w}_{c}x_{ci}) + \lambda \sum_{j}\sum_{v \in V} s_v\parallel \mathbf{w}^j_{G_v}\parallel_2,
\end{split}
\end{equation}
与上文中介绍的方法相比，基于树形结构约束的多任务学习在对于任务分组和任务信息分享上更加的灵活，它不仅仅允许属于不同组的任务共享信息，还通过对每个组加入权重使每个任务都得到均衡的惩罚，然而它的缺点在于任务之间的组的结构信息属于先验知识，即需要先用一种距离度量出任务之间的相似性，再用聚类方法得到树形结构，而其他方法可以同时优化出任务的结构和最终的模型的参数矩阵。

\begin{figure}
  \centering
  \subfigure[ ]{
    \label{p8} %% label for first subfigure
    \includegraphics[width=3in]{8.eps}}
  \hspace{1in}
  \subfigure[ ]{
    \label{p9} %% label for second subfigure
    \includegraphics[width=1.9in]{9.eps}}
  \caption{(a)任务之间存在树形结构，圆圈表示任务，方块表示组。同属于同一个组的任务相似度比较高。(b)任务之间存在树形结构，圆圈表示任务，方块表示组。同属于同一个组的任务相似度比较高。}
  \label{fig:subfig} %% label for entire figure
\end{figure}

\Exe 尽管基于树形结构约束的多任务学习在任务分组和信息共享十分灵活，但是大部分情况下任务的结构还是单层的。基于此，Dinesh Jayaraman~\cite{JayaramanSG14} 对基于树形结构约束的多任务学习做出了改进，提出了一种既可以灵活共享特征又是单层的组结构的多任务学习方法，并将其用于属性学习，在属性学习中将每个属性视为任务，且属性之间具有显而易见的组结构。例如描述颜色的属性同属一组，它们之间共享描述颜色的特征。这个目标通过对任务组内加以~$\ell_{2,1}$~约束和组间加以~$\ell_{1}$~约束的方式实现，该方法的具体的目标函数如下：

\begin{equation}
\begin{split}
\label{f8}
    \min \sum_{c=1}^C\sum_{i=1}^{N_c} L(y_{ti},\mathbf{w}_{c}\mathbf{x}_{ci}) + \lambda \sum_{d=1}^D\sum_{v \in V} \parallel \mathbf{w}^d_{G_v}\parallel_2,
\end{split}
\end{equation}
这里~$\mathbf{w}^d_{G_v}$~是一个行向量，它代表了属于第~$v$~组中所有任务对于第~$d$~维特征的参数向量。第二项正则项对~$\mathbf{W}$~ 矩阵逐行惩罚，先在每个任务组内计算~$\ell_{2,1}$~范数，再在组间计算~$\ell_1$~范数，从而鼓励同属于一组内的任务选择相同的特征维数，达到组内共享与组间竞争的期望。如果数据库中的~$C$~个任务都属于同一个组，即数据库中只有一个任务组，此时~$G_1 = {1, 2,..., C}$~，组结构约束也退化为~$\ell_{2,1}$~范数，公式~\eqref{f8}~相应的也退化为~2.3.1~小节中的基于~$\ell_{2,1}$~范数的多任务学习框架，鼓励所有任务学习到一个特征空间。如果数据中的的~$C$~个任务互不相关，那么就有~$C$~个任务组~${G_1,G_2,...G_C}$~，每个组内只包含一个任务。此时组结构约束退化为~$\ell_1$~范数，公式~\eqref{f8}~将为所有的任务学习出各不相同的稀疏的特征表示。

\section{本章小结}

单任务学习为每个任务学习独立的模型，忽略了任务之间的关系。多任务学习通过利用任务之间共享的信息弥补了单任务学习的这一缺陷，并且提高了模型的性能和泛化能力。目前为止研究者们提出了很多的多任务学习方法框架，主要分为基于~$\ell_{2,1}$~范数的多任务学习和基于组约束的多任务学习。前者也可以称为联合学习的多任务学习方法，因为它为所有的任务联合学习同一个低维度的特征空间并鼓励所有任务之间都分享特征信息。后者认为基于~$\ell_{2,1}$~范数的多任务学习方法不适用于绝大多数的现实情况。于是研究者们提出了各种方法来探索任务之间的结构信息并用它们来约束多任务学习的过程。其中典型的有RMTFL等单层结构模型，还有树形结构和图模型结构等多层模型。这一类方法将数据库中的任务分为若干组，相似的任务位于同一组。同一组内的任务存在特征共享，而不同组内的任务存在特征竞争。
